---
title: "Homework 3"
date: 2022-10-17
categories:
  - homework
tags:
  - distributions
  - csv
  - c#
  - online
  - mean
---
<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

# (T) Research on Theory
### Illustrate the concept of joint, marginal and conditional relative frequency using a simple bivariate distribution.

Let us use the following table, depicting the bivariate distribution of eye color and hair color:

|Absolute<br>frequencies|blue|brown|green
|-|--|--|--|
|black |3|4|1
|blonde|4|2|3
|red   |1|2|0
|brown |2|6|2

By dividing each cell by the total, we get the joint distribution of the two variables: each cell now represents the probability of observing an individual with both that eye color and hair color.

|Joint<br>distribution|blue|brown|green
|-|--|--|--|
|black |10.00%|13.33%| 3.33%
|blonde|13.33%| 6.66%|10.00%
|red   | 3.33%| 6.66%| 0.00%
|brown | 6.66%|20.00%| 6.66%

Now, by restricting our table on a row or column, and re-normalizing the resulting distribution, we can extrapolate the conditional distribution relative to a value of a variable.

|Conditional<br>distribution|green|
|-|--|
|black |16.66%
|blonde|50.00%
|red   | 0.00%
|brown |33.33%

*This is the original distribution conditioned on the people with green eyes*.

From the joint distribution, we can also compute the marginal distributions, by summing up the relative frequencies on each row/column and re-normalizing.

||blue|brown|green|HairColor
|-|--|--|--|--|
|black   |10.00%|13.33%| 3.33%|26.66%
|blonde  |13.33%| 6.66%|10.00%|30.00%
|red     | 3.33%| 6.66%| 0.00%|10.00%
|brown   | 6.66%|   20%| 6.66%|33.33%
|**EyeColor**|33.33%|46.66%|20.00%

### Discuss the concept of statistical independence and the mathematical relationship between independence and relative frequencies.
Two variables $$x$$ and $$y$$ are independent if the distribution of $$x$$ does not change if conditioned over any value of $$y$$.

In probability we say that $$x$$ and $$y$$ are independent if: 
    $$\forall v_1\in\text{Im}(x),\quad\forall v_2\in\text{Im}(y)$$:
    $$\mathbb P[x=v_1]=\mathbb P[x=v_1|y=v_2] \land \mathbb P[y=v_2]=\mathbb P[y=v_2|x=v_1]$$

That directly implies that if two variables are independent, then their bivariate distribution table has the following property: all rows are proportional to each other and all columns are proportional to each other. That is because, after cutting a single row or column (for computing the conditional) and re-normalizing, the resulting conditional distributions cannot be different from each other, otherwise we wouldn't have independent variables.


# (A) Application
### Create a distribution from the data obtained with the program Wireshark reading the CSV file.
It turns out that Wireshark outputs CSV files in a very standard format, which means that analyzing them with the program presented in homework 2 is pretty straight-forward.

Here is the computed distribution of packet's protocols when browsing the web.

![Browsing the Web]({{ site.baseurl }}/assets/hw3/image-1.png)

Here is the computed distribution of packet's protocols when watching a video stream.

![Browsing the Web]({{ site.baseurl }}/assets/hw3/image-2.png)



# (TA) Research on Application
### A survey on online algorithms.
Online algorithms are algorithms that can only see their input in a streaming fashion, without being able to see them in their entirety from the start. They work by analyzing their input piece by piece and constructing their output as they read their input.

Online algorithms have been devised for solving many problems, like computing summaries of data series (like averages and variance), but they are also applied to fields like machine learning, in Online Learning algorithms.


**References**:
+ [Online Algorithms -- Wikipedia](https://en.wikipedia.org/wiki/Online_algorithm)

### Knuth recursion for the computation of the arithmetic mean.
One such algorithm, is a more numerically stable version of the algorithm for computing the arithmetic mean, the Knuth algorithm.

The formula for the arithmetic mean the following:

$$M=\dfrac1n\sum_{i=1}^n x_i=\sum_{i=1}^n \dfrac{x_i}n$$

Knuth modifies it to be a series of outputs: $$\bar x_k$$, such that $$\bar x_1=x_1$$ and $$\bar x_n=M$$. To compute the average more efficiently, each new value of $$\bar x_k$$ is computed using the previous one.

By definition, since $$\bar x_k=\frac 1k \sum_{i=1}^k x_i$$, then 

$$\bar x_{k+1}=\dfrac{k\bar x_k + x_{k+1}}{k+1}=\bar x_k\dfrac{x_{k+1}-\bar x_k}{k+1}$$

It is trivial to prove by induction that the formula holds and always returns the true average of the series.
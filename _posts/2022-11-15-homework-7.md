---
title: "Homework 7"
date: 2022-11-15
categories:
  - homework
tags:
  - c#
  - lebesgue
  - law of large numbers
---
<script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

# (T) Research on Theory
### Try to understand the general idea of the Lebesgue-Stieltjes integral and why it is useful concept and notation in Theory of Probability
The idea behind the **Lebesgue-Stieltjes** integral is to integrate over the image of a function rather than its domain. This helps when integrating functions that have an infinite amount of discontinuities as well as making the definition of integral more general.

The **Lebesgue-Stieltjes** integral is based on the concept of measure, a fundamental item in the Measure Theory (and also in probability theory).

The reason why it is useful in the Theory of Probability is that it generalizes any definition that is based on the distribution of a random variable (such as the expectation, variance, etc.). This means that instead of differentiating between discrete and continuous random variables, we can just use the **Lebesgue-Stieltjes** integral to define such concepts for any random variable.


### Explain in your own words the "Law of large numbers" and sketch a simple proof (Markov inequality, Čebyšëv inequality, ...)

The **Law of large numbers** states that the average of a large number of independent and identically distributed random variables will converge their expected value.

We can easily prove its weak form by using the **Markov inequality**.

Let $$X_1, X_2, \dots, X_n$$ be a sequence of independent and identically distributed random variables with expected value $$\mu$$ and variance $$\sigma^2$$. Let $$\bar{X}_n = \frac{1}{n} \sum_{i=1}^n X_i$$ be the sample mean of the first $$n$$ random variables. Its mean will be $$\mu$$ and its variance will be $$\frac{\sigma^2}{n}$$ as seen in the previous homework.


Then, by the **Čebyšëv inequality** we have that:

$$\forall \varepsilon>0, \mathbb P(|\bar{X}_n - \mu| \geq \varepsilon) \leq \frac{\sigma^2}{n \varepsilon^2}$$

From this we can state that the probability that the sample mean is within $$\varepsilon$$ of the expected value is at least $$1 - \frac{\sigma^2}{n \varepsilon^2}$$.

So, for any $$\varepsilon > 0$$, we have that:

$$\mathbb P(|\bar{X}_n - \mu| \leq \varepsilon) \geq 1 - \frac{\sigma^2}{n \varepsilon^2}$$

So, as $$n \to \infty$$, we have that:

$$\forall \varepsilon > 0, \lim_{n \to \infty} \mathbb P(|\bar{X}_n - \mu| \leq \varepsilon) = 1$$

This means that the sample mean will converge to $$\mu$$ as $$n \to \infty$$ independently of the $$\varepsilon$$ we choose (the convergence is in probability).

# (A) Application
### Consider a general scheme ad the simulation of previous homework and simulate the distribution of $$p = \sum(x_i)$$ , where $$x_i$$ are $$\text{Bernoulli}(\lambda/n)$$, with success probability $$\lambda/n$$, where $$\lambda$$ is a user-defined constant ("arrival rate"). Also plot the distribution of the "inter-arrival times".
